# RunPod vLLM Worker Environment Variables
# Copy these to your RunPod Serverless endpoint configuration

# Required: Model configuration
MODEL_NAME=google/medgemma-27b-text-it

# Required: Hugging Face access token (for gated models)
HF_TOKEN=your_huggingface_token_here

# Optional: Model context length (adjust based on model capabilities)
MAX_MODEL_LEN=8192

# Optional: GPU memory utilization (0.0 to 1.0)
GPU_MEMORY_UTILIZATION=0.95

# Optional: Maximum concurrent requests
MAX_CONCURRENCY=30

# Optional: Quantization method (for lower memory GPUs)
# Options: "awq", "gptq", "squeezellm", "bitsandbytes"
# QUANTIZATION=

# Optional: Number of GPUs for tensor parallelism
# TENSOR_PARALLEL_SIZE=1

# Optional: Maximum number of sequences per iteration
# MAX_NUM_SEQS=256

# Optional: Custom chat template (Jinja2 format)
# CUSTOM_CHAT_TEMPLATE=

# Optional: Override model name in OpenAI API responses
# OPENAI_SERVED_MODEL_NAME_OVERRIDE=medgemma-27b
